{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yNKLu7pRVBF"
   },
   "source": [
    "필수 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVBjcZ6IRHZH"
   },
   "outputs": [],
   "source": [
    "# !pip install -U transformers bitsandbytes accelerate sentencepiece -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qS43f-9KPKjv"
   },
   "source": [
    "#### bitsandbytes 라이브러리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QBlRktV-dt-"
   },
   "source": [
    "비트샌드바이트는 CUDA 커스텀 함수, 특히 8비트 옵티마이저, 행렬 곱셈(LLM.int8()), 양자화 함수에 대한 경량 래퍼입니다.\n",
    "\n",
    "The bitsandbytes is a lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTeIP2SoT9lE"
   },
   "source": [
    "# 1.4비트 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wAAX_LOBPF9s"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dab5c39eb8c4b3e8516c0c1edd52220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21876f2493e4c3796200b025da3d4a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "model_id = \"bigscience/bloom-1b7\"\n",
    "# model_id = \"gpt2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_4bit=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T8wdB8z1Bbql"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.632878592\n"
     ]
    }
   ],
   "source": [
    "# 1.63GB로 양자화함\n",
    "print(model.get_memory_footprint() / 1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.88963584\n"
     ]
    }
   ],
   "source": [
    "# 원래 모델 크기\n",
    "print(model.get_memory_footprint() / 1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3idtyIOuT8kB"
   },
   "source": [
    "# 2.8비트 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7fLllWS5T0c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_8bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XRtWfXEGUIwc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.236858368\n"
     ]
    }
   ],
   "source": [
    "# 원래 모델 크기\n",
    "print(model.get_memory_footprint() / 1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nDDhwGYFIyM"
   },
   "source": [
    "# 3.16비트 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dEfQOvwEFIRB"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "aLnyvjpYGLnE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.44481792\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./../bloom-lb7-ft16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8bWhPp7WGf4p"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\").half() # 메모리를 쿠다로 옮길때, 16비트로 옮김\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "FnwhO6WxGjnz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.44481792\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYpRoG4HWJP3"
   },
   "source": [
    "# 4.기타 용법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "li198qYhWMyW"
   },
   "source": [
    "## 4.1. compute_dtype 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8eOccmWCvZg"
   },
   "source": [
    "compute_dtype은 계산 중에 사용할 dtype을 변경하는 데 사용됩니다.\n",
    "\n",
    "예를 들어 은닉상태를 float32로 설정할 수 있지만 속도를 높이려면 계산을 bf16으로 설정할 수 있습니다.\n",
    "\n",
    "기본적으로 compute dtype은 float32로 설정됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "pItqhudmVC3C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_4bit=True,\n",
    "    device_map='auto',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6oVuNlBWdKL"
   },
   "source": [
    "## 4.2. NF4 (Normal Float 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- float4로 경량화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-_KSqXfgVsXL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_4bit=True,\n",
    "    device_map='auto',\n",
    "    bnb_4bit_quant_type='nf4'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.632878592\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEByF193WxuZ"
   },
   "source": [
    "## 4.3. 중첩 양자화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Zaluga0HLJf"
   },
   "source": [
    "또한 사용자에게 중첩 양자화 기법을 사용할 것을 권장합니다. 이 기법을 사용하면 추가 성능 없이 더 많은 메모리를 절약할 수 있습니다. 경험적 관찰에 따르면 이 기법을 사용하면 시퀀스 길이 1024, 배치 크기 1, 그라데이션 누적 단계 4로 NVIDIA-T4 16GB에서 라마-13b 모델을 미세 조정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "lg0uHTBBWhO5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_id = \"bigscience/bloom-1b7\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map='auto',\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "auEc-NWnHRnw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.632878592\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ugnNE_3eKMY"
   },
   "source": [
    "## 4.4. `cpu` 와 `gpu` 오프로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gpu가 부족할 때, cpu를 활용하여 모델을 올림\n",
    "- 하단의 device_map에서 layer마다 cpu/gpu 지정 가능 (0은 gpu, 'cpu'는 cpu를 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0VTh8yqCd4fl"
   },
   "outputs": [],
   "source": [
    "device_map = {\n",
    "    \"transformer.word_embeddings\": 0,\n",
    "    \"transformer.word_embeddings_layernorm\": 0,\n",
    "    \"lm_head\": \"cpu\",\n",
    "    \"transformer.h\": 0,\n",
    "    \"transformer.ln_f\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "cj6i5uMSeBhh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tied parameters are on different devices: {'lm_head.weight': 'cpu', 'transformer.word_embeddings.weight': 0}. Please modify your custom device map or set `device_map='auto'`. \n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bigscience/bloom-1b7\",\n",
    "    device_map=device_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "IYs6oMaEg5k-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(250880, 2048)\n",
       "    (word_embeddings_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=250880, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lij4zC4eJZHw"
   },
   "source": [
    "# 5.130억 파라마터 모델 로딩 및 추론하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Qx0k2xLhhWre"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f379bfc1ed2d460ab17fc14a0c7d9d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.22 s, sys: 19.6 s, total: 24.8 s\n",
      "Wall time: 9min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_path = \"./../LLM/open_llama_13b/\"\n",
    "# model_id = \"openlm-research/open_llama_13b\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "a7i_qKEKJvZC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.04202752\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "BJTtFTWUL5jz"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Bui3r9OqLurg"
   },
   "outputs": [],
   "source": [
    "# 1) 질의 문장\n",
    "input_text = \"What is deep learning?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "Bui3r9OqLurg"
   },
   "outputs": [],
   "source": [
    "# 2) 토큰화하고 텐서로 변환합니다.\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "# input_ids = input_ids.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Bui3r9OqLurg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cslee/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:1460: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "/home/cslee/anaconda3/envs/llm/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3) 생성 옵션을 설정하고 텍스트를 생성합니다.\n",
    "max_length = 200\n",
    "sample_outputs = model.generate(input_ids, do_sample=True, max_length=max_length, temperature=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Bui3r9OqLurg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is deep learning?\n",
      "Kaplan, Alex\n",
      "The term “deep learning” can be confusing because it is applied to a variety of research fields, each with their own goals and applications.\n",
      "This article will be divided into the three main areas of deep learning research: 1) Representation learning, 2) Deep neural networks, and 3) Unsupervised learning.\n",
      "Each area will be described in depth, with links to other resources and a bibliography to the major papers cited in this article.\n",
      "Deep Learning and Representation Learning\n",
      "Deep learning is a particular form of representation learning, in which the goal is to represent a complex, high-dimensional input as a low-dimensional representation.\n",
      "Deep learning is applied in three major areas:\n",
      "1) Computer vision\n",
      "2) Natural language processing\n",
      "3) Speech recognition\n",
      "The term “deep learning” was coined in 2006 by Geoffrey Hinton. The term was\n"
     ]
    }
   ],
   "source": [
    "# 4) 생성된 텍스트를 디코딩합니다.\n",
    "\n",
    "print(tokenizer.decode(sample_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1oZysoybUCKV9BeL-w3hkMnJ80byfVnYG",
     "timestamp": 1713018440535
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
